Abstract

Earthâ€™s climate has changed over the course of history. Within the last 650,000 years, there have been seven cycles of glacial advance and retreat. With the ending of the last ice age about 11,700 years ago, the modern climate era and human civilization began. The climate is now changing faster than at any point in the past, primarily due to human activities. Global climate change has already resulted in a wide range of impacts across the globe and many sectors of the economy that are expected to grow in the coming decades. The goal of the first assignment is to develop a focused web crawler to build a collection of blog posts and articles about climate change.

Introduction

As climate change transforms global ecosystems, it affects everything from the places we live to the water we drink to the air we breathe. The failure to mitigate and adapt to climate change is the most impactful risk facing communities worldwide - ahead of weapons of mass destruction and water crises. My goal throughout this project is to better understand the issues surrounding the climate change topic, from history to evidence, causes, effects, scientific consensus, and public opinion.
For the first assignment specifically, I aim to create a corpus containing at least 300 documents, each of which is written in English and has at least 300 words. The sections of this paper includes Literature Review which reviews similar work done by others, Methods which discusses how I will be conducting my research, Results which lays out what I have learned from this research, and Conclusions which summarizes the findings and discusses the next step.

Literature review

With the rapid growth of network information, the Internet has become the most popular source of data. However, to get the relevant knowledge of interest in an efficient way from a vast amount of information is not easy and has become a hot topic in the current research. This task is also known as crawling web pages. Web crawlers are programs that collect information from the internet. There are two types of web crawlers: general-purpose and special-purpose. General-purpose web crawler collects and processes the entire contents online in a centralized location, so that it can be indexed in advance to be able to respond to many queries. A special-purpose web crawler, or focused crawler, collects web pages that satisfy some specific requirements, and yields good recall as well as good precision (Lu, Zhan, Zhou, and He 2016). However, in the article Searching the Web by Arasu, Cho, Garcia-Molina, Paepcke, and Raghavan (2001), the authors mentioned that generic crawlers are more essential in reality as focused crawlers require prior knowledge from users and are bound to vary from case to case.

Method

To collect documents, I built a focused crawler using Scrapy and followed the eight activities suggested in the article focused on crawling by Chakrabarti, van den Berg, and Dom (1999), the winner of the Best Paper award at the Eighth International World Wide Web Conference. The first step was canonical topic/taxonomy creation. I picked environmental issues as the general topic of interest.. The second step was to collect examples of relevant documents. I manually selected 40 webpages that covered different aspects of the main topic. The third step was to narrow the topic down. I decided to focus on climate change and global warming, so I refined the original set to 17 web pages that were most suitable. The fourth step was interactive exploration of the web. For this part, I performed a few initial crawls to see how things work and adjusted the code as needed. The fifth step was classifier training, which is to develop a text classification model that predicts page relevance after downloading web pages. The sixth step was resource discovery. From the selected 17 web pages, I wrote the code for the crawler to follow all of the links found on them and scrape the text from the second layer of webpages. The seventh step was distiller definition and development. That means building a model to predict page relevance before downloading. The last step was user evaluation/feedback. I inspected the system, revised the code, and repeated the process several times until it was good enough.
I did not explore much about the fifth step (classifer) and the seventh step (distiller) due to limited time. However, if I were to build a distiller, I would try to identify the most influential nodes by means of network analysis, using indices such as eigenvector centrality. For a classifier, I would develop a model using Term Frequency Inverse Document Frequency (TFIDF), which is known to be the most common approach of term weighting in text classification problems.

Result

After the web crawler finished scraping, there were 1901 documents downloaded. To make sure that all documents were at least 300 words, I generated a csv file, imported it into a Pandas dataframe, and filtered the length. The final corpus consisted of 1178 documents. All of them are written in meaningful English paragraphs and sentences. Information about url, title, and body are also generated.

Conclusion

Though it was the first assignment, I learned a lot about Scrapy and how to extract data from websites. It is a powerful package but definitely difficult to master. What I found most interesting is its ability to extract other links from the original sources, which helps reduce the amount of time looking for extra information. Looking at the documents, I saw that the majority of them were relevant to the topic of climate change and global warming. However, for the web scraping project in the future, I will go deeper into developing a distiller and a classifier to guarantee the higher quality of the corpus.
